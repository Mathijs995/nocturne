{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nocturne concepts\n",
    "\n",
    "This page introduces the most basic elements of nocturne. You can find further information about these [in Section 3 of the paper](https://arxiv.org/abs/2206.09889).\n",
    "\n",
    "_Last update: April 2023_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_path = 'data/example_scenario.json'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- Nocturne simulations are discretized traffic scenarios. A scenario is a constructed snapshot of traffic situation at a particular timepoint.\n",
    "- Vehicles observe the traffic scene from their own viewpoint and therefore the state of every vehicle is unique. We call the state of the vehicle we focus on the ego state.\n",
    "- Nocturne incorporates physical principles in rendering objects in the traffic scene. `#TODO`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation\n",
    "\n",
    "In Nocturne, a simulation discretizes an existing traffic scenario. At the moment, Nocturne supports traffic scenarios from the Waymo Open Dataset, but can be further extended to work with other driving datasets. \n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src='https://drive.google.com/uc?id=1nv5Rbyf7ZfdqTdaUduXvEI7ncdkLpDjc' width='600'/>\n",
    "<figcaption></figcaption>An example of a set of traffic scenario's in Nocturne. Upon initialization, a start time is chosen. After each iteration we take a step in the simulation, which gets us to the next scenario. This is done until we reach the end of the simulation (last image). </center>\n",
    "</figure>\n",
    "\n",
    "We show an example of this using `example_scenario.json`, where our traffic data is extracted from the Waymo open motion dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nocturne import Simulation\n",
    "\n",
    "scenario_config = {\n",
    "    'start_time': 0, # When to start the simulation\n",
    "    'allow_non_vehicles': True, # Whether to include cyclists and pedestrians \n",
    "    'max_visible_road_points': 1, # Maximum number of road points for a vehicle\n",
    "    'max_visible_objects': 1, # Maximum number of road objects for a vehicle\n",
    "    'max_visible_traffic_lights': 1,\n",
    "    'max_visible_stop_signs': 1,\n",
    "}\n",
    "\n",
    "# Create simulation\n",
    "sim = Simulation(data_path, scenario_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario\n",
    "\n",
    "A simulation consists of a set of scenarios. A scenario is a snapshot of the traffic scene at a particular timepoint. \n",
    "\n",
    "Here is how to create a scenario object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get traffic scenario at timepoint\n",
    "scenario = sim.getScenario()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scenario` objects holds information we are interested in. Here are a couple of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of road objects in the scene\n",
    "len(scenario.getObjects())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # moving objects: 15\n",
      "\n",
      "Object IDs of moving vehicles: \n",
      " [0, 1, 2, 3, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32] \n"
     ]
    }
   ],
   "source": [
    "# The road objects that moved at a particular timepoint\n",
    "objects_that_moved = scenario.getObjectsThatMoved()\n",
    "\n",
    "print(f'Total # moving objects: {len(objects_that_moved)}\\n')\n",
    "print(f'Object IDs of moving vehicles: \\n {[obj.getID() for obj in objects_that_moved]} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of road lines\n",
    "len(scenario.road_lines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<nocturne_cpp.Vehicle at 0x117537030>,\n",
       " <nocturne_cpp.Vehicle at 0x10ec140b0>,\n",
       " <nocturne_cpp.Vehicle at 0x1175288f0>,\n",
       " <nocturne_cpp.Vehicle at 0x117537430>,\n",
       " <nocturne_cpp.Vehicle at 0x1175376f0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenario.getVehicles()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 moving vehicles in scene: [3, 32]\n"
     ]
    }
   ],
   "source": [
    "# Select all moving vehicles that move \n",
    "moving_vehicles = [obj for obj in scenario.getVehicles() if obj in objects_that_moved]\n",
    "\n",
    "print(f'Found {len(moving_vehicles)} moving vehicles in scene: {[vehicle.getID() for vehicle in moving_vehicles]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ego state\n",
    "\n",
    "The **ego state** is an array with features that describe the current vehicle. This array holds the following information: \n",
    "- 0: length of ego vehicle\n",
    "- 1: width of ego vehicle\n",
    "- 2: speed of ego vehicle\n",
    "- 3: distance to the goal position of ego vehicle\n",
    "- 4: angle to the goal (target azimuth) \n",
    "- 5: desired heading at goal position\n",
    "- 6: desired speed at goal position\n",
    "- 7: current acceleration\n",
    "- 8: current steering position\n",
    "- 9: current head angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected vehicle # 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 4.4936213 ,  1.9770377 ,  0.07662283,  4.24219   , -0.05617166,\n",
       "       -0.05909407,  1.6792779 ,  0.        ,  0.        ,  0.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select an arbitrary vehicle\n",
    "ego_vehicle = moving_vehicles[0]\n",
    "\n",
    "print(f'Selected vehicle # {ego_vehicle.getID()}')\n",
    "\n",
    "# Get the state for ego vehicle\n",
    "scenario.ego_state(ego_vehicle)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visible state\n",
    "\n",
    "We use the ego vehicle state, together with a view distance (how far the vehicle can see) and a view angle to construct the **visible state**. The figure below shows this procedure for a simplified traffic scene. \n",
    "\n",
    "The visible state comprises four matrices:\n",
    "- `stop_signs`: the visible stop signs of `(1, 13)`\n",
    "- `traffic_lights`: the states for the traffic lights from the perspective of the ego driver.\n",
    "- `road_points`: the observable road points (static elements in the scene)\n",
    "- `objects`: the observable road objects (vehicles, pedestrians and cyclists)\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src='https://drive.google.com/uc?id=1fG43NvPCzaimmW99asRdB73qY-F4u-q0' width='700'/>\n",
    "<figcaption>To investigate coordination under partial observability, agents in Nocturne can only see an obstructed view of their environment. In this simplified traffic scene, we construct the state for the red ego driver. Note that Nocturne assumes that stop signs can be viewed, even if they are behind another driver. </figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['stop_signs', 'traffic_lights', 'road_points', 'objects'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define viewing distance, radius and head angle\n",
    "view_distance = 80 \n",
    "view_angle = np.radians(120) \n",
    "head_angle = 0\n",
    "\n",
    "# Construct the visible state for ego vehicle\n",
    "visible_state = scenario.visible_state(\n",
    "    ego_vehicle, \n",
    "    view_dist=view_distance, \n",
    "    view_angle=view_angle,\n",
    "    head_angle=head_angle,\n",
    ")\n",
    "\n",
    "visible_state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 3), dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No visible stop signs\n",
    "visible_state['stop_signs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 12), dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty; are filtered out in this version of Nocturne\n",
    "visible_state['traffic_lights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 13)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max visible road points x 13 features\n",
    "visible_state['road_points'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 13)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of visible road objects x features \n",
    "visible_state['objects'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features for 'stop_signs' = 3\n",
      "Number of features for 'traffic_lights' = 12\n",
      "Number of features for 'road_points' = 13\n",
      "Number of features for 'objects' = 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension = 0\n",
    "\n",
    "for key, val in visible_state.items():\n",
    "    print(f\"Number of features for '{key}' = {val.shape[1]}\")\n",
    "\n",
    "    dimension += val.shape[0] * val.shape[1]\n",
    "\n",
    "dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also flatten the visible state\n",
    "visible_state_flat = scenario.flattened_visible_state(\n",
    "        ego_vehicle, view_dist=view_distance, view_angle=view_angle, head_angle=head_angle\n",
    ")\n",
    "\n",
    "visible_state_flat.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step \n",
    "\n",
    "`step(dt)` is a method call on an instance of the Simulation class, where `dt` is a scalar that represents the length of each simulation timestep in seconds. It advances the simulation by one timestep, which can result in changes to the state of the simulation (for example, new positions of objects, updated velocities, etc.) based on the physical laws and rules defined in the simulation.\n",
    "\n",
    "In the Waymo dataset, the length of the expert data is 9 seconds, and we use a step size of 0.1. The first second is used as a warm-start, leaving the remaining 8 seconds (80 steps) for the simulation (details in Section 3.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.1\n",
    "\n",
    "# Step the simulation\n",
    "sim.step(dt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action space\n",
    "\n",
    "The action set for a vehicle consists of three components: acceleration, steering and the head angle. Actions are discretized based on a provided upper and lower bound.\n",
    "\n",
    "In the experiments in the paper we use:\n",
    "- 6 discrete actions for **acceleration** uniformly split between $[-3, 2] \\, \\frac{m}{s^2}$\n",
    "- 21 discrete actions for **steering** between $[-0.7, 0.7]$ radians \n",
    "- 5 discrete actions for **head tilt** between $[-1.6, 1.6]$ radians\n",
    "\n",
    "Here is you can access the action for a vehicle Nocturne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{acceleration: -0.224648, steering: -0.360994, head_angle: 0.000000}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose an arbitrary timepoint\n",
    "time = 5\n",
    "\n",
    "# Show expert action at timepoint\n",
    "scenario.expert_action(ego_vehicle, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nocturne_cpp.Action"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scenario.expert_action(ego_vehicle, time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.005859, 0.004639)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How did the vehicle's position change after taking this action?\n",
    "scenario.expert_pos_shift(ego_vehicle, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0007097125053405762"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How did the head angle change?\n",
    "scenario.expert_heading_shift(ego_vehicle, time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nocturne-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
